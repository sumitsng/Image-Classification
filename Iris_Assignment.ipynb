{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sumit Singhal - Iris_Assignment_new.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumitsng/Image-Classification/blob/master/Iris_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h67nVXf2zl-0",
        "colab_type": "text"
      },
      "source": [
        "### Import Iris Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VRnLqfGzl-2",
        "colab_type": "code",
        "outputId": "3d4643b3-b1e9-42e8-fd06-09351bc50a92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2750
        }
      },
      "source": [
        "import sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "import keras\n",
        "np.random.seed(10)\n",
        "iris = load_iris()\n",
        "print(iris)\n",
        "#print(iris.data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
            "       [4.9, 3. , 1.4, 0.2],\n",
            "       [4.7, 3.2, 1.3, 0.2],\n",
            "       [4.6, 3.1, 1.5, 0.2],\n",
            "       [5. , 3.6, 1.4, 0.2],\n",
            "       [5.4, 3.9, 1.7, 0.4],\n",
            "       [4.6, 3.4, 1.4, 0.3],\n",
            "       [5. , 3.4, 1.5, 0.2],\n",
            "       [4.4, 2.9, 1.4, 0.2],\n",
            "       [4.9, 3.1, 1.5, 0.1],\n",
            "       [5.4, 3.7, 1.5, 0.2],\n",
            "       [4.8, 3.4, 1.6, 0.2],\n",
            "       [4.8, 3. , 1.4, 0.1],\n",
            "       [4.3, 3. , 1.1, 0.1],\n",
            "       [5.8, 4. , 1.2, 0.2],\n",
            "       [5.7, 4.4, 1.5, 0.4],\n",
            "       [5.4, 3.9, 1.3, 0.4],\n",
            "       [5.1, 3.5, 1.4, 0.3],\n",
            "       [5.7, 3.8, 1.7, 0.3],\n",
            "       [5.1, 3.8, 1.5, 0.3],\n",
            "       [5.4, 3.4, 1.7, 0.2],\n",
            "       [5.1, 3.7, 1.5, 0.4],\n",
            "       [4.6, 3.6, 1. , 0.2],\n",
            "       [5.1, 3.3, 1.7, 0.5],\n",
            "       [4.8, 3.4, 1.9, 0.2],\n",
            "       [5. , 3. , 1.6, 0.2],\n",
            "       [5. , 3.4, 1.6, 0.4],\n",
            "       [5.2, 3.5, 1.5, 0.2],\n",
            "       [5.2, 3.4, 1.4, 0.2],\n",
            "       [4.7, 3.2, 1.6, 0.2],\n",
            "       [4.8, 3.1, 1.6, 0.2],\n",
            "       [5.4, 3.4, 1.5, 0.4],\n",
            "       [5.2, 4.1, 1.5, 0.1],\n",
            "       [5.5, 4.2, 1.4, 0.2],\n",
            "       [4.9, 3.1, 1.5, 0.2],\n",
            "       [5. , 3.2, 1.2, 0.2],\n",
            "       [5.5, 3.5, 1.3, 0.2],\n",
            "       [4.9, 3.6, 1.4, 0.1],\n",
            "       [4.4, 3. , 1.3, 0.2],\n",
            "       [5.1, 3.4, 1.5, 0.2],\n",
            "       [5. , 3.5, 1.3, 0.3],\n",
            "       [4.5, 2.3, 1.3, 0.3],\n",
            "       [4.4, 3.2, 1.3, 0.2],\n",
            "       [5. , 3.5, 1.6, 0.6],\n",
            "       [5.1, 3.8, 1.9, 0.4],\n",
            "       [4.8, 3. , 1.4, 0.3],\n",
            "       [5.1, 3.8, 1.6, 0.2],\n",
            "       [4.6, 3.2, 1.4, 0.2],\n",
            "       [5.3, 3.7, 1.5, 0.2],\n",
            "       [5. , 3.3, 1.4, 0.2],\n",
            "       [7. , 3.2, 4.7, 1.4],\n",
            "       [6.4, 3.2, 4.5, 1.5],\n",
            "       [6.9, 3.1, 4.9, 1.5],\n",
            "       [5.5, 2.3, 4. , 1.3],\n",
            "       [6.5, 2.8, 4.6, 1.5],\n",
            "       [5.7, 2.8, 4.5, 1.3],\n",
            "       [6.3, 3.3, 4.7, 1.6],\n",
            "       [4.9, 2.4, 3.3, 1. ],\n",
            "       [6.6, 2.9, 4.6, 1.3],\n",
            "       [5.2, 2.7, 3.9, 1.4],\n",
            "       [5. , 2. , 3.5, 1. ],\n",
            "       [5.9, 3. , 4.2, 1.5],\n",
            "       [6. , 2.2, 4. , 1. ],\n",
            "       [6.1, 2.9, 4.7, 1.4],\n",
            "       [5.6, 2.9, 3.6, 1.3],\n",
            "       [6.7, 3.1, 4.4, 1.4],\n",
            "       [5.6, 3. , 4.5, 1.5],\n",
            "       [5.8, 2.7, 4.1, 1. ],\n",
            "       [6.2, 2.2, 4.5, 1.5],\n",
            "       [5.6, 2.5, 3.9, 1.1],\n",
            "       [5.9, 3.2, 4.8, 1.8],\n",
            "       [6.1, 2.8, 4. , 1.3],\n",
            "       [6.3, 2.5, 4.9, 1.5],\n",
            "       [6.1, 2.8, 4.7, 1.2],\n",
            "       [6.4, 2.9, 4.3, 1.3],\n",
            "       [6.6, 3. , 4.4, 1.4],\n",
            "       [6.8, 2.8, 4.8, 1.4],\n",
            "       [6.7, 3. , 5. , 1.7],\n",
            "       [6. , 2.9, 4.5, 1.5],\n",
            "       [5.7, 2.6, 3.5, 1. ],\n",
            "       [5.5, 2.4, 3.8, 1.1],\n",
            "       [5.5, 2.4, 3.7, 1. ],\n",
            "       [5.8, 2.7, 3.9, 1.2],\n",
            "       [6. , 2.7, 5.1, 1.6],\n",
            "       [5.4, 3. , 4.5, 1.5],\n",
            "       [6. , 3.4, 4.5, 1.6],\n",
            "       [6.7, 3.1, 4.7, 1.5],\n",
            "       [6.3, 2.3, 4.4, 1.3],\n",
            "       [5.6, 3. , 4.1, 1.3],\n",
            "       [5.5, 2.5, 4. , 1.3],\n",
            "       [5.5, 2.6, 4.4, 1.2],\n",
            "       [6.1, 3. , 4.6, 1.4],\n",
            "       [5.8, 2.6, 4. , 1.2],\n",
            "       [5. , 2.3, 3.3, 1. ],\n",
            "       [5.6, 2.7, 4.2, 1.3],\n",
            "       [5.7, 3. , 4.2, 1.2],\n",
            "       [5.7, 2.9, 4.2, 1.3],\n",
            "       [6.2, 2.9, 4.3, 1.3],\n",
            "       [5.1, 2.5, 3. , 1.1],\n",
            "       [5.7, 2.8, 4.1, 1.3],\n",
            "       [6.3, 3.3, 6. , 2.5],\n",
            "       [5.8, 2.7, 5.1, 1.9],\n",
            "       [7.1, 3. , 5.9, 2.1],\n",
            "       [6.3, 2.9, 5.6, 1.8],\n",
            "       [6.5, 3. , 5.8, 2.2],\n",
            "       [7.6, 3. , 6.6, 2.1],\n",
            "       [4.9, 2.5, 4.5, 1.7],\n",
            "       [7.3, 2.9, 6.3, 1.8],\n",
            "       [6.7, 2.5, 5.8, 1.8],\n",
            "       [7.2, 3.6, 6.1, 2.5],\n",
            "       [6.5, 3.2, 5.1, 2. ],\n",
            "       [6.4, 2.7, 5.3, 1.9],\n",
            "       [6.8, 3. , 5.5, 2.1],\n",
            "       [5.7, 2.5, 5. , 2. ],\n",
            "       [5.8, 2.8, 5.1, 2.4],\n",
            "       [6.4, 3.2, 5.3, 2.3],\n",
            "       [6.5, 3. , 5.5, 1.8],\n",
            "       [7.7, 3.8, 6.7, 2.2],\n",
            "       [7.7, 2.6, 6.9, 2.3],\n",
            "       [6. , 2.2, 5. , 1.5],\n",
            "       [6.9, 3.2, 5.7, 2.3],\n",
            "       [5.6, 2.8, 4.9, 2. ],\n",
            "       [7.7, 2.8, 6.7, 2. ],\n",
            "       [6.3, 2.7, 4.9, 1.8],\n",
            "       [6.7, 3.3, 5.7, 2.1],\n",
            "       [7.2, 3.2, 6. , 1.8],\n",
            "       [6.2, 2.8, 4.8, 1.8],\n",
            "       [6.1, 3. , 4.9, 1.8],\n",
            "       [6.4, 2.8, 5.6, 2.1],\n",
            "       [7.2, 3. , 5.8, 1.6],\n",
            "       [7.4, 2.8, 6.1, 1.9],\n",
            "       [7.9, 3.8, 6.4, 2. ],\n",
            "       [6.4, 2.8, 5.6, 2.2],\n",
            "       [6.3, 2.8, 5.1, 1.5],\n",
            "       [6.1, 2.6, 5.6, 1.4],\n",
            "       [7.7, 3. , 6.1, 2.3],\n",
            "       [6.3, 3.4, 5.6, 2.4],\n",
            "       [6.4, 3.1, 5.5, 1.8],\n",
            "       [6. , 3. , 4.8, 1.8],\n",
            "       [6.9, 3.1, 5.4, 2.1],\n",
            "       [6.7, 3.1, 5.6, 2.4],\n",
            "       [6.9, 3.1, 5.1, 2.3],\n",
            "       [5.8, 2.7, 5.1, 1.9],\n",
            "       [6.8, 3.2, 5.9, 2.3],\n",
            "       [6.7, 3.3, 5.7, 2.5],\n",
            "       [6.7, 3. , 5.2, 2.3],\n",
            "       [6.3, 2.5, 5. , 1.9],\n",
            "       [6.5, 3. , 5.2, 2. ],\n",
            "       [6.2, 3.4, 5.4, 2.3],\n",
            "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': '/usr/local/lib/python3.6/dist-packages/sklearn/datasets/data/iris.csv'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfIOp7dMzl-6",
        "colab_type": "text"
      },
      "source": [
        "### Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loF7rJdEzl-7",
        "colab_type": "code",
        "outputId": "07764672-36ed-4627-f6cb-d7ae67f87f17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Print type of iris\n",
        "print(type(iris))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'sklearn.utils.Bunch'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym9QkrIkzl-9",
        "colab_type": "code",
        "outputId": "51ec4f1c-29e2-45d1-afce-51b007429c10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Print parent classes of this type\n",
        "## Use the attribute __bases__ of the type that you have found\n",
        "print(sklearn.utils.Bunch.__base__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wURPkVU1zl_A",
        "colab_type": "text"
      },
      "source": [
        "So, you must have found that iris is a dict object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-56ZsrFzl_B",
        "colab_type": "code",
        "outputId": "3b873b41-008f-4e46-8718-72eed3924044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Print the keys present in iris\n",
        "print(iris.keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jnt3Sv7Uzl_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Create X and Y variables. X is the input features, Y is the output labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLW3fvJaNzd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=[]\n",
        "Y=[]\n",
        "X=iris.data\n",
        "Y=iris.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7pWhWCUNzED",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6_GvO1UNyTz",
        "colab_type": "code",
        "outputId": "4d81302f-bfd8-4512-a28a-9db49829f44e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "Y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3svVv6RFzl_F",
        "colab_type": "code",
        "outputId": "cf95671c-779f-40dd-8c26-0b50f056b8d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Track 5 random samples\n",
        "isamples = np.random.randint(len(Y), size = (5))\n",
        "print(isamples)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  9 125  15  64 113]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh6Npv8izl_K",
        "colab_type": "code",
        "outputId": "e2b16b10-054e-457a-9a91-70dd4e8d32d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#Shape of Data\n",
        "print(iris.data.shape)\n",
        "# Print shapes of X and Y\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 4)\n",
            "(150, 4)\n",
            "(150,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbrqSQ55zl_M",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhC_m1KCzl_N",
        "colab_type": "text"
      },
      "source": [
        "#### Convert Labels(Y) to one-hot encoding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZCkBgU8zl_N",
        "colab_type": "code",
        "outputId": "cd4fda1e-65f5-421e-ee66-d0d437bc06e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "### Convert Y to 1-hot encoding\n",
        "\n",
        "##Your Code Here\n",
        "def oneHot(y, Ny):\n",
        "    '''\n",
        "    Input:\n",
        "        y: an int in {0, 1, 2}\n",
        "        Ny: Number of classes, e.g., 3 here.\n",
        "    Output:\n",
        "        Y: a vector of Ny (=3) tuples\n",
        "    '''\n",
        "    Y = np.zeros(Ny)\n",
        "    Y[y] = 1\n",
        "    return Y\n",
        "Ny = 3\n",
        "Y= np.array([oneHot(y,Ny) for y in Y])\n",
        "\n",
        "# Look at converted Y for the random samples we are t\n",
        "print(Y[isamples])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pZraxUHzl_R",
        "colab_type": "text"
      },
      "source": [
        "### Its time to split your model into training and testing samples "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZGJDkkNzl_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " from sklearn.model_selection import train_test_split\n",
        "### split the data into 80% training and 20% testing\n",
        "### Split X into two variables X_train and X_test\n",
        "### Split Y correspondingly into two variables Y_train and Y_test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEYivGTUzl_V",
        "colab_type": "text"
      },
      "source": [
        "### Data Normalization/Scaling\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHnqhqwdzl_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Normalize data to be of zero mean and unit variance\n",
        "def findMeanStddev(X):\n",
        "    '''\n",
        "    Input: \n",
        "        X: a matrix of size (no. of samples, dimension of each sample)\n",
        "    Output:\n",
        "        mean: mean of samples in X (same size as X)\n",
        "        stddev: element-wise std dev of sample in X (same size as X)\n",
        "    '''\n",
        "    mean = np.sum(X, axis=0)/X.shape[0]\n",
        "    X1 = X-mean\n",
        "    stddev = np.sqrt(np.sum(X1*X1, axis=0)/X.shape[0])\n",
        "    return mean, stddev\n",
        "\n",
        "def normalizeX(X, mean, stddev):\n",
        "    '''\n",
        "    Input:\n",
        "        X: a matrix of size (no. of samples, dimension of each sample)\n",
        "        mean: mean of samples in X (same size as X)\n",
        "        stddev: element-wise std dev of sample in X (same size as X) \n",
        "    Output:\n",
        "        Xn: X modified to have 0 mean and 1 std dev\n",
        "    '''\n",
        "    Xn = (X-mean)/(stddev+1e-8)\n",
        "    return Xn\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOB4xDdBzl_a",
        "colab_type": "text"
      },
      "source": [
        "Note: If the first 5 samples of X_train and Y_train looks like the one given below\n",
        "then congratulations! You are on the right track;\n",
        "If not, then persistance is the key to success :)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFkYJFduzl_c",
        "colab_type": "code",
        "outputId": "050a384b-9f1b-49c5-b97c-61b96747826c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2609
        }
      },
      "source": [
        "## Print and observe the normalized inputs and compare with older inputs\n",
        "mean_train, stddev_train = findMeanStddev(X)\n",
        "X = normalizeX(X, mean_train, stddev_train)\n",
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-9.00681159e-01  1.01900433e+00 -1.34022652e+00 -1.31544428e+00]\n",
            " [-1.14301690e+00 -1.31979476e-01 -1.34022652e+00 -1.31544428e+00]\n",
            " [-1.38535264e+00  3.28414046e-01 -1.39706395e+00 -1.31544428e+00]\n",
            " [-1.50652050e+00  9.82172847e-02 -1.28338909e+00 -1.31544428e+00]\n",
            " [-1.02184903e+00  1.24920109e+00 -1.34022652e+00 -1.31544428e+00]\n",
            " [-5.37177552e-01  1.93979137e+00 -1.16971424e+00 -1.05217991e+00]\n",
            " [-1.50652050e+00  7.88807568e-01 -1.34022652e+00 -1.18381210e+00]\n",
            " [-1.02184903e+00  7.88807568e-01 -1.28338909e+00 -1.31544428e+00]\n",
            " [-1.74885624e+00 -3.62176237e-01 -1.34022652e+00 -1.31544428e+00]\n",
            " [-1.14301690e+00  9.82172847e-02 -1.28338909e+00 -1.44707646e+00]\n",
            " [-5.37177552e-01  1.47939785e+00 -1.28338909e+00 -1.31544428e+00]\n",
            " [-1.26418477e+00  7.88807568e-01 -1.22655167e+00 -1.31544428e+00]\n",
            " [-1.26418477e+00 -1.31979476e-01 -1.34022652e+00 -1.44707646e+00]\n",
            " [-1.87002411e+00 -1.31979476e-01 -1.51073880e+00 -1.44707646e+00]\n",
            " [-5.25060766e-02  2.16998813e+00 -1.45390137e+00 -1.31544428e+00]\n",
            " [-1.73673946e-01  3.09077518e+00 -1.28338909e+00 -1.05217991e+00]\n",
            " [-5.37177552e-01  1.93979137e+00 -1.39706395e+00 -1.05217991e+00]\n",
            " [-9.00681159e-01  1.01900433e+00 -1.34022652e+00 -1.18381210e+00]\n",
            " [-1.73673946e-01  1.70959461e+00 -1.16971424e+00 -1.18381210e+00]\n",
            " [-9.00681159e-01  1.70959461e+00 -1.28338909e+00 -1.18381210e+00]\n",
            " [-5.37177552e-01  7.88807568e-01 -1.16971424e+00 -1.31544428e+00]\n",
            " [-9.00681159e-01  1.47939785e+00 -1.28338909e+00 -1.05217991e+00]\n",
            " [-1.50652050e+00  1.24920109e+00 -1.56757623e+00 -1.31544428e+00]\n",
            " [-9.00681159e-01  5.58610807e-01 -1.16971424e+00 -9.20547730e-01]\n",
            " [-1.26418477e+00  7.88807568e-01 -1.05603939e+00 -1.31544428e+00]\n",
            " [-1.02184903e+00 -1.31979476e-01 -1.22655167e+00 -1.31544428e+00]\n",
            " [-1.02184903e+00  7.88807568e-01 -1.22655167e+00 -1.05217991e+00]\n",
            " [-7.79513290e-01  1.01900433e+00 -1.28338909e+00 -1.31544428e+00]\n",
            " [-7.79513290e-01  7.88807568e-01 -1.34022652e+00 -1.31544428e+00]\n",
            " [-1.38535264e+00  3.28414046e-01 -1.22655167e+00 -1.31544428e+00]\n",
            " [-1.26418477e+00  9.82172847e-02 -1.22655167e+00 -1.31544428e+00]\n",
            " [-5.37177552e-01  7.88807568e-01 -1.28338909e+00 -1.05217991e+00]\n",
            " [-7.79513290e-01  2.40018489e+00 -1.28338909e+00 -1.44707646e+00]\n",
            " [-4.16009683e-01  2.63038166e+00 -1.34022652e+00 -1.31544428e+00]\n",
            " [-1.14301690e+00  9.82172847e-02 -1.28338909e+00 -1.31544428e+00]\n",
            " [-1.02184903e+00  3.28414046e-01 -1.45390137e+00 -1.31544428e+00]\n",
            " [-4.16009683e-01  1.01900433e+00 -1.39706395e+00 -1.31544428e+00]\n",
            " [-1.14301690e+00  1.24920109e+00 -1.34022652e+00 -1.44707646e+00]\n",
            " [-1.74885624e+00 -1.31979476e-01 -1.39706395e+00 -1.31544428e+00]\n",
            " [-9.00681159e-01  7.88807568e-01 -1.28338909e+00 -1.31544428e+00]\n",
            " [-1.02184903e+00  1.01900433e+00 -1.39706395e+00 -1.18381210e+00]\n",
            " [-1.62768837e+00 -1.74335680e+00 -1.39706395e+00 -1.18381210e+00]\n",
            " [-1.74885624e+00  3.28414046e-01 -1.39706395e+00 -1.31544428e+00]\n",
            " [-1.02184903e+00  1.01900433e+00 -1.22655167e+00 -7.88915547e-01]\n",
            " [-9.00681159e-01  1.70959461e+00 -1.05603939e+00 -1.05217991e+00]\n",
            " [-1.26418477e+00 -1.31979476e-01 -1.34022652e+00 -1.18381210e+00]\n",
            " [-9.00681159e-01  1.70959461e+00 -1.22655167e+00 -1.31544428e+00]\n",
            " [-1.50652050e+00  3.28414046e-01 -1.34022652e+00 -1.31544428e+00]\n",
            " [-6.58345421e-01  1.47939785e+00 -1.28338909e+00 -1.31544428e+00]\n",
            " [-1.02184903e+00  5.58610807e-01 -1.34022652e+00 -1.31544428e+00]\n",
            " [ 1.40150835e+00  3.28414046e-01  5.35408558e-01  2.64141913e-01]\n",
            " [ 6.74501137e-01  3.28414046e-01  4.21733705e-01  3.95774096e-01]\n",
            " [ 1.28034048e+00  9.82172847e-02  6.49083412e-01  3.95774096e-01]\n",
            " [-4.16009683e-01 -1.74335680e+00  1.37546572e-01  1.32509730e-01]\n",
            " [ 7.95669006e-01 -5.92372998e-01  4.78571132e-01  3.95774096e-01]\n",
            " [-1.73673946e-01 -5.92372998e-01  4.21733705e-01  1.32509730e-01]\n",
            " [ 5.53333268e-01  5.58610807e-01  5.35408558e-01  5.27406278e-01]\n",
            " [-1.14301690e+00 -1.51316004e+00 -2.60315414e-01 -2.62386817e-01]\n",
            " [ 9.16836875e-01 -3.62176237e-01  4.78571132e-01  1.32509730e-01]\n",
            " [-7.79513290e-01 -8.22569759e-01  8.07091458e-02  2.64141913e-01]\n",
            " [-1.02184903e+00 -2.43394709e+00 -1.46640561e-01 -2.62386817e-01]\n",
            " [ 6.86617924e-02 -1.31979476e-01  2.51221426e-01  3.95774096e-01]\n",
            " [ 1.89829661e-01 -1.97355356e+00  1.37546572e-01 -2.62386817e-01]\n",
            " [ 3.10997530e-01 -3.62176237e-01  5.35408558e-01  2.64141913e-01]\n",
            " [-2.94841815e-01 -3.62176237e-01 -8.98031340e-02  1.32509730e-01]\n",
            " [ 1.03800474e+00  9.82172847e-02  3.64896279e-01  2.64141913e-01]\n",
            " [-2.94841815e-01 -1.31979476e-01  4.21733705e-01  3.95774096e-01]\n",
            " [-5.25060766e-02 -8.22569759e-01  1.94383999e-01 -2.62386817e-01]\n",
            " [ 4.32165399e-01 -1.97355356e+00  4.21733705e-01  3.95774096e-01]\n",
            " [-2.94841815e-01 -1.28296328e+00  8.07091458e-02 -1.30754635e-01]\n",
            " [ 6.86617924e-02  3.28414046e-01  5.92245985e-01  7.90670643e-01]\n",
            " [ 3.10997530e-01 -5.92372998e-01  1.37546572e-01  1.32509730e-01]\n",
            " [ 5.53333268e-01 -1.28296328e+00  6.49083412e-01  3.95774096e-01]\n",
            " [ 3.10997530e-01 -5.92372998e-01  5.35408558e-01  8.77547884e-04]\n",
            " [ 6.74501137e-01 -3.62176237e-01  3.08058852e-01  1.32509730e-01]\n",
            " [ 9.16836875e-01 -1.31979476e-01  3.64896279e-01  2.64141913e-01]\n",
            " [ 1.15917261e+00 -5.92372998e-01  5.92245985e-01  2.64141913e-01]\n",
            " [ 1.03800474e+00 -1.31979476e-01  7.05920838e-01  6.59038461e-01]\n",
            " [ 1.89829661e-01 -3.62176237e-01  4.21733705e-01  3.95774096e-01]\n",
            " [-1.73673946e-01 -1.05276652e+00 -1.46640561e-01 -2.62386817e-01]\n",
            " [-4.16009683e-01 -1.51316004e+00  2.38717192e-02 -1.30754635e-01]\n",
            " [-4.16009683e-01 -1.51316004e+00 -3.29657074e-02 -2.62386817e-01]\n",
            " [-5.25060766e-02 -8.22569759e-01  8.07091458e-02  8.77547884e-04]\n",
            " [ 1.89829661e-01 -8.22569759e-01  7.62758265e-01  5.27406278e-01]\n",
            " [-5.37177552e-01 -1.31979476e-01  4.21733705e-01  3.95774096e-01]\n",
            " [ 1.89829661e-01  7.88807568e-01  4.21733705e-01  5.27406278e-01]\n",
            " [ 1.03800474e+00  9.82172847e-02  5.35408558e-01  3.95774096e-01]\n",
            " [ 5.53333268e-01 -1.74335680e+00  3.64896279e-01  1.32509730e-01]\n",
            " [-2.94841815e-01 -1.31979476e-01  1.94383999e-01  1.32509730e-01]\n",
            " [-4.16009683e-01 -1.28296328e+00  1.37546572e-01  1.32509730e-01]\n",
            " [-4.16009683e-01 -1.05276652e+00  3.64896279e-01  8.77547884e-04]\n",
            " [ 3.10997530e-01 -1.31979476e-01  4.78571132e-01  2.64141913e-01]\n",
            " [-5.25060766e-02 -1.05276652e+00  1.37546572e-01  8.77547884e-04]\n",
            " [-1.02184903e+00 -1.74335680e+00 -2.60315414e-01 -2.62386817e-01]\n",
            " [-2.94841815e-01 -8.22569759e-01  2.51221426e-01  1.32509730e-01]\n",
            " [-1.73673946e-01 -1.31979476e-01  2.51221426e-01  8.77547884e-04]\n",
            " [-1.73673946e-01 -3.62176237e-01  2.51221426e-01  1.32509730e-01]\n",
            " [ 4.32165399e-01 -3.62176237e-01  3.08058852e-01  1.32509730e-01]\n",
            " [-9.00681159e-01 -1.28296328e+00 -4.30827694e-01 -1.30754635e-01]\n",
            " [-1.73673946e-01 -5.92372998e-01  1.94383999e-01  1.32509730e-01]\n",
            " [ 5.53333268e-01  5.58610807e-01  1.27429510e+00  1.71209592e+00]\n",
            " [-5.25060766e-02 -8.22569759e-01  7.62758265e-01  9.22302826e-01]\n",
            " [ 1.52267622e+00 -1.31979476e-01  1.21745768e+00  1.18556719e+00]\n",
            " [ 5.53333268e-01 -3.62176237e-01  1.04694540e+00  7.90670643e-01]\n",
            " [ 7.95669006e-01 -1.31979476e-01  1.16062025e+00  1.31719937e+00]\n",
            " [ 2.12851557e+00 -1.31979476e-01  1.61531966e+00  1.18556719e+00]\n",
            " [-1.14301690e+00 -1.28296328e+00  4.21733705e-01  6.59038461e-01]\n",
            " [ 1.76501196e+00 -3.62176237e-01  1.44480738e+00  7.90670643e-01]\n",
            " [ 1.03800474e+00 -1.28296328e+00  1.16062025e+00  7.90670643e-01]\n",
            " [ 1.64384409e+00  1.24920109e+00  1.33113253e+00  1.71209592e+00]\n",
            " [ 7.95669006e-01  3.28414046e-01  7.62758265e-01  1.05393501e+00]\n",
            " [ 6.74501137e-01 -8.22569759e-01  8.76433118e-01  9.22302826e-01]\n",
            " [ 1.15917261e+00 -1.31979476e-01  9.90107971e-01  1.18556719e+00]\n",
            " [-1.73673946e-01 -1.28296328e+00  7.05920838e-01  1.05393501e+00]\n",
            " [-5.25060766e-02 -5.92372998e-01  7.62758265e-01  1.58046374e+00]\n",
            " [ 6.74501137e-01  3.28414046e-01  8.76433118e-01  1.44883156e+00]\n",
            " [ 7.95669006e-01 -1.31979476e-01  9.90107971e-01  7.90670643e-01]\n",
            " [ 2.24968343e+00  1.70959461e+00  1.67215709e+00  1.31719937e+00]\n",
            " [ 2.24968343e+00 -1.05276652e+00  1.78583194e+00  1.44883156e+00]\n",
            " [ 1.89829661e-01 -1.97355356e+00  7.05920838e-01  3.95774096e-01]\n",
            " [ 1.28034048e+00  3.28414046e-01  1.10378282e+00  1.44883156e+00]\n",
            " [-2.94841815e-01 -5.92372998e-01  6.49083412e-01  1.05393501e+00]\n",
            " [ 2.24968343e+00 -5.92372998e-01  1.67215709e+00  1.05393501e+00]\n",
            " [ 5.53333268e-01 -8.22569759e-01  6.49083412e-01  7.90670643e-01]\n",
            " [ 1.03800474e+00  5.58610807e-01  1.10378282e+00  1.18556719e+00]\n",
            " [ 1.64384409e+00  3.28414046e-01  1.27429510e+00  7.90670643e-01]\n",
            " [ 4.32165399e-01 -5.92372998e-01  5.92245985e-01  7.90670643e-01]\n",
            " [ 3.10997530e-01 -1.31979476e-01  6.49083412e-01  7.90670643e-01]\n",
            " [ 6.74501137e-01 -5.92372998e-01  1.04694540e+00  1.18556719e+00]\n",
            " [ 1.64384409e+00 -1.31979476e-01  1.16062025e+00  5.27406278e-01]\n",
            " [ 1.88617983e+00 -5.92372998e-01  1.33113253e+00  9.22302826e-01]\n",
            " [ 2.49201917e+00  1.70959461e+00  1.50164481e+00  1.05393501e+00]\n",
            " [ 6.74501137e-01 -5.92372998e-01  1.04694540e+00  1.31719937e+00]\n",
            " [ 5.53333268e-01 -5.92372998e-01  7.62758265e-01  3.95774096e-01]\n",
            " [ 3.10997530e-01 -1.05276652e+00  1.04694540e+00  2.64141913e-01]\n",
            " [ 2.24968343e+00 -1.31979476e-01  1.33113253e+00  1.44883156e+00]\n",
            " [ 5.53333268e-01  7.88807568e-01  1.04694540e+00  1.58046374e+00]\n",
            " [ 6.74501137e-01  9.82172847e-02  9.90107971e-01  7.90670643e-01]\n",
            " [ 1.89829661e-01 -1.31979476e-01  5.92245985e-01  7.90670643e-01]\n",
            " [ 1.28034048e+00  9.82172847e-02  9.33270545e-01  1.18556719e+00]\n",
            " [ 1.03800474e+00  9.82172847e-02  1.04694540e+00  1.58046374e+00]\n",
            " [ 1.28034048e+00  9.82172847e-02  7.62758265e-01  1.44883156e+00]\n",
            " [-5.25060766e-02 -8.22569759e-01  7.62758265e-01  9.22302826e-01]\n",
            " [ 1.15917261e+00  3.28414046e-01  1.21745768e+00  1.44883156e+00]\n",
            " [ 1.03800474e+00  5.58610807e-01  1.10378282e+00  1.71209592e+00]\n",
            " [ 1.03800474e+00 -1.31979476e-01  8.19595691e-01  1.44883156e+00]\n",
            " [ 5.53333268e-01 -1.28296328e+00  7.05920838e-01  9.22302826e-01]\n",
            " [ 7.95669006e-01 -1.31979476e-01  8.19595691e-01  1.05393501e+00]\n",
            " [ 4.32165399e-01  7.88807568e-01  9.33270545e-01  1.44883156e+00]\n",
            " [ 6.86617924e-02 -1.31979476e-01  7.62758265e-01  7.90670643e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlV6-O6yzl_f",
        "colab_type": "text"
      },
      "source": [
        "### Training time (using Neural Networks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVMF86uBzl_h",
        "colab_type": "text"
      },
      "source": [
        "- create your NN model with given structure (4 input neurons, 8 hidden neurons and 3 output neurons)\n",
        "- use 'relu' activation in hidden layer \n",
        "- use softmax activation in output\n",
        "- Compile your model using 'categorical_crossentropy' loss function and sgd optimizer\n",
        "- Track the accuracy metric of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLnW8eDizl_i",
        "colab_type": "code",
        "outputId": "ce50c50f-c0f9-43a7-eb10-ea82ca4091a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "# fix random seed for reproducibility\n",
        "#numpy.random.seed(7)\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(8, input_dim=4, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL22uaU5zl_r",
        "colab_type": "code",
        "outputId": "7cab5d47-a11d-4758-8ad5-852128ba8359",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "## Print the model summary\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 8)                 40        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 27        \n",
            "=================================================================\n",
            "Total params: 67\n",
            "Trainable params: 67\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX1SPWt5zl_v",
        "colab_type": "code",
        "outputId": "d6d46c27-5c76-496f-c98a-12cecdd4b4d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5290
        }
      },
      "source": [
        "## Fit the model to data\n",
        "model.fit(X_train, Y_train, epochs=150, batch_size=10)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 1.4798 - acc: 0.3167\n",
            "Epoch 2/150\n",
            "120/120 [==============================] - 0s 212us/step - loss: 0.9998 - acc: 0.3000\n",
            "Epoch 3/150\n",
            "120/120 [==============================] - 0s 186us/step - loss: 0.9147 - acc: 0.5000\n",
            "Epoch 4/150\n",
            "120/120 [==============================] - 0s 166us/step - loss: 0.8781 - acc: 0.5167\n",
            "Epoch 5/150\n",
            "120/120 [==============================] - 0s 184us/step - loss: 0.8573 - acc: 0.7250\n",
            "Epoch 6/150\n",
            "120/120 [==============================] - 0s 197us/step - loss: 0.8252 - acc: 0.7000\n",
            "Epoch 7/150\n",
            "120/120 [==============================] - 0s 186us/step - loss: 0.7859 - acc: 0.7667\n",
            "Epoch 8/150\n",
            "120/120 [==============================] - 0s 183us/step - loss: 0.7554 - acc: 0.7750\n",
            "Epoch 9/150\n",
            "120/120 [==============================] - 0s 176us/step - loss: 0.7219 - acc: 0.7833\n",
            "Epoch 10/150\n",
            "120/120 [==============================] - 0s 170us/step - loss: 0.6919 - acc: 0.8000\n",
            "Epoch 11/150\n",
            "120/120 [==============================] - 0s 166us/step - loss: 0.6623 - acc: 0.8083\n",
            "Epoch 12/150\n",
            "120/120 [==============================] - 0s 192us/step - loss: 0.6417 - acc: 0.8750\n",
            "Epoch 13/150\n",
            "120/120 [==============================] - 0s 158us/step - loss: 0.6181 - acc: 0.8417\n",
            "Epoch 14/150\n",
            "120/120 [==============================] - 0s 184us/step - loss: 0.5908 - acc: 0.8500\n",
            "Epoch 15/150\n",
            "120/120 [==============================] - 0s 178us/step - loss: 0.5710 - acc: 0.8667\n",
            "Epoch 16/150\n",
            "120/120 [==============================] - 0s 179us/step - loss: 0.5551 - acc: 0.8917\n",
            "Epoch 17/150\n",
            "120/120 [==============================] - 0s 188us/step - loss: 0.5380 - acc: 0.9167\n",
            "Epoch 18/150\n",
            "120/120 [==============================] - 0s 180us/step - loss: 0.5204 - acc: 0.8583\n",
            "Epoch 19/150\n",
            "120/120 [==============================] - 0s 180us/step - loss: 0.5040 - acc: 0.9083\n",
            "Epoch 20/150\n",
            "120/120 [==============================] - 0s 172us/step - loss: 0.4953 - acc: 0.9000\n",
            "Epoch 21/150\n",
            "120/120 [==============================] - 0s 167us/step - loss: 0.4848 - acc: 0.9083\n",
            "Epoch 22/150\n",
            "120/120 [==============================] - 0s 129us/step - loss: 0.4731 - acc: 0.9000\n",
            "Epoch 23/150\n",
            "120/120 [==============================] - 0s 151us/step - loss: 0.4640 - acc: 0.9167\n",
            "Epoch 24/150\n",
            "120/120 [==============================] - 0s 185us/step - loss: 0.4600 - acc: 0.9000\n",
            "Epoch 25/150\n",
            "120/120 [==============================] - 0s 159us/step - loss: 0.4436 - acc: 0.9083\n",
            "Epoch 26/150\n",
            "120/120 [==============================] - 0s 158us/step - loss: 0.4335 - acc: 0.9250\n",
            "Epoch 27/150\n",
            "120/120 [==============================] - 0s 184us/step - loss: 0.4200 - acc: 0.9167\n",
            "Epoch 28/150\n",
            "120/120 [==============================] - 0s 184us/step - loss: 0.4170 - acc: 0.9250\n",
            "Epoch 29/150\n",
            "120/120 [==============================] - 0s 184us/step - loss: 0.4078 - acc: 0.9417\n",
            "Epoch 30/150\n",
            "120/120 [==============================] - 0s 170us/step - loss: 0.4071 - acc: 0.9167\n",
            "Epoch 31/150\n",
            "120/120 [==============================] - 0s 178us/step - loss: 0.4015 - acc: 0.8833\n",
            "Epoch 32/150\n",
            "120/120 [==============================] - 0s 156us/step - loss: 0.3917 - acc: 0.9167\n",
            "Epoch 33/150\n",
            "120/120 [==============================] - 0s 188us/step - loss: 0.3789 - acc: 0.9417\n",
            "Epoch 34/150\n",
            "120/120 [==============================] - 0s 166us/step - loss: 0.3779 - acc: 0.9500\n",
            "Epoch 35/150\n",
            "120/120 [==============================] - 0s 172us/step - loss: 0.3712 - acc: 0.9083\n",
            "Epoch 36/150\n",
            "120/120 [==============================] - 0s 176us/step - loss: 0.3627 - acc: 0.9500\n",
            "Epoch 37/150\n",
            "120/120 [==============================] - 0s 176us/step - loss: 0.3567 - acc: 0.9583\n",
            "Epoch 38/150\n",
            "120/120 [==============================] - 0s 168us/step - loss: 0.3587 - acc: 0.9333\n",
            "Epoch 39/150\n",
            "120/120 [==============================] - 0s 176us/step - loss: 0.3485 - acc: 0.9500\n",
            "Epoch 40/150\n",
            "120/120 [==============================] - 0s 188us/step - loss: 0.3502 - acc: 0.9000\n",
            "Epoch 41/150\n",
            "120/120 [==============================] - 0s 157us/step - loss: 0.3396 - acc: 0.9583\n",
            "Epoch 42/150\n",
            "120/120 [==============================] - 0s 182us/step - loss: 0.3363 - acc: 0.9667\n",
            "Epoch 43/150\n",
            "120/120 [==============================] - 0s 180us/step - loss: 0.3351 - acc: 0.9250\n",
            "Epoch 44/150\n",
            "120/120 [==============================] - 0s 228us/step - loss: 0.3341 - acc: 0.9583\n",
            "Epoch 45/150\n",
            "120/120 [==============================] - 0s 150us/step - loss: 0.3178 - acc: 0.9667\n",
            "Epoch 46/150\n",
            "120/120 [==============================] - 0s 210us/step - loss: 0.3198 - acc: 0.9500\n",
            "Epoch 47/150\n",
            "120/120 [==============================] - 0s 163us/step - loss: 0.3162 - acc: 0.9250\n",
            "Epoch 48/150\n",
            "120/120 [==============================] - 0s 294us/step - loss: 0.3149 - acc: 0.9417\n",
            "Epoch 49/150\n",
            "120/120 [==============================] - 0s 188us/step - loss: 0.2993 - acc: 0.9750\n",
            "Epoch 50/150\n",
            "120/120 [==============================] - 0s 187us/step - loss: 0.3067 - acc: 0.9583\n",
            "Epoch 51/150\n",
            "120/120 [==============================] - 0s 167us/step - loss: 0.2927 - acc: 0.9417\n",
            "Epoch 52/150\n",
            "120/120 [==============================] - 0s 174us/step - loss: 0.2851 - acc: 0.9583\n",
            "Epoch 53/150\n",
            "120/120 [==============================] - 0s 168us/step - loss: 0.2969 - acc: 0.9583\n",
            "Epoch 54/150\n",
            "120/120 [==============================] - 0s 168us/step - loss: 0.2814 - acc: 0.9583\n",
            "Epoch 55/150\n",
            "120/120 [==============================] - 0s 206us/step - loss: 0.2733 - acc: 0.9500\n",
            "Epoch 56/150\n",
            "120/120 [==============================] - 0s 167us/step - loss: 0.2775 - acc: 0.9167\n",
            "Epoch 57/150\n",
            "120/120 [==============================] - 0s 166us/step - loss: 0.2787 - acc: 0.9250\n",
            "Epoch 58/150\n",
            "120/120 [==============================] - 0s 145us/step - loss: 0.2759 - acc: 0.9667\n",
            "Epoch 59/150\n",
            "120/120 [==============================] - 0s 162us/step - loss: 0.2663 - acc: 0.9667\n",
            "Epoch 60/150\n",
            "120/120 [==============================] - 0s 174us/step - loss: 0.2635 - acc: 0.9667\n",
            "Epoch 61/150\n",
            "120/120 [==============================] - 0s 167us/step - loss: 0.2600 - acc: 0.9500\n",
            "Epoch 62/150\n",
            "120/120 [==============================] - 0s 146us/step - loss: 0.2556 - acc: 0.9667\n",
            "Epoch 63/150\n",
            "120/120 [==============================] - 0s 189us/step - loss: 0.2538 - acc: 0.9667\n",
            "Epoch 64/150\n",
            "120/120 [==============================] - 0s 199us/step - loss: 0.2498 - acc: 0.9417\n",
            "Epoch 65/150\n",
            "120/120 [==============================] - 0s 176us/step - loss: 0.2586 - acc: 0.9583\n",
            "Epoch 66/150\n",
            "120/120 [==============================] - 0s 167us/step - loss: 0.2457 - acc: 0.9750\n",
            "Epoch 67/150\n",
            "120/120 [==============================] - 0s 176us/step - loss: 0.2455 - acc: 0.9667\n",
            "Epoch 68/150\n",
            "120/120 [==============================] - 0s 172us/step - loss: 0.2378 - acc: 0.9583\n",
            "Epoch 69/150\n",
            "120/120 [==============================] - 0s 171us/step - loss: 0.2403 - acc: 0.9667\n",
            "Epoch 70/150\n",
            "120/120 [==============================] - 0s 186us/step - loss: 0.2304 - acc: 0.9667\n",
            "Epoch 71/150\n",
            "120/120 [==============================] - 0s 179us/step - loss: 0.2248 - acc: 0.9500\n",
            "Epoch 72/150\n",
            "120/120 [==============================] - 0s 168us/step - loss: 0.2242 - acc: 0.9833\n",
            "Epoch 73/150\n",
            "120/120 [==============================] - 0s 166us/step - loss: 0.2179 - acc: 0.9667\n",
            "Epoch 74/150\n",
            "120/120 [==============================] - 0s 187us/step - loss: 0.2131 - acc: 0.9750\n",
            "Epoch 75/150\n",
            "120/120 [==============================] - 0s 175us/step - loss: 0.2210 - acc: 0.9750\n",
            "Epoch 76/150\n",
            "120/120 [==============================] - 0s 191us/step - loss: 0.2238 - acc: 0.9583\n",
            "Epoch 77/150\n",
            "120/120 [==============================] - 0s 182us/step - loss: 0.2113 - acc: 0.9667\n",
            "Epoch 78/150\n",
            "120/120 [==============================] - 0s 177us/step - loss: 0.2148 - acc: 0.9750\n",
            "Epoch 79/150\n",
            "120/120 [==============================] - 0s 177us/step - loss: 0.2202 - acc: 0.9500\n",
            "Epoch 80/150\n",
            "120/120 [==============================] - 0s 185us/step - loss: 0.2060 - acc: 0.9667\n",
            "Epoch 81/150\n",
            "120/120 [==============================] - 0s 213us/step - loss: 0.2030 - acc: 0.9500\n",
            "Epoch 82/150\n",
            "120/120 [==============================] - 0s 168us/step - loss: 0.2071 - acc: 0.9750\n",
            "Epoch 83/150\n",
            "120/120 [==============================] - 0s 157us/step - loss: 0.1949 - acc: 0.9750\n",
            "Epoch 84/150\n",
            "120/120 [==============================] - 0s 200us/step - loss: 0.1982 - acc: 0.9667\n",
            "Epoch 85/150\n",
            "120/120 [==============================] - 0s 201us/step - loss: 0.1919 - acc: 0.9750\n",
            "Epoch 86/150\n",
            "120/120 [==============================] - 0s 239us/step - loss: 0.2007 - acc: 0.9583\n",
            "Epoch 87/150\n",
            "120/120 [==============================] - 0s 253us/step - loss: 0.1933 - acc: 0.9667\n",
            "Epoch 88/150\n",
            "120/120 [==============================] - 0s 176us/step - loss: 0.1940 - acc: 0.9750\n",
            "Epoch 89/150\n",
            "120/120 [==============================] - 0s 171us/step - loss: 0.1881 - acc: 0.9583\n",
            "Epoch 90/150\n",
            "120/120 [==============================] - 0s 181us/step - loss: 0.1869 - acc: 0.9750\n",
            "Epoch 91/150\n",
            "120/120 [==============================] - 0s 240us/step - loss: 0.1881 - acc: 0.9750\n",
            "Epoch 92/150\n",
            "120/120 [==============================] - 0s 187us/step - loss: 0.1955 - acc: 0.9500\n",
            "Epoch 93/150\n",
            "120/120 [==============================] - 0s 199us/step - loss: 0.1818 - acc: 0.9667\n",
            "Epoch 94/150\n",
            "120/120 [==============================] - 0s 158us/step - loss: 0.1788 - acc: 0.9750\n",
            "Epoch 95/150\n",
            "120/120 [==============================] - 0s 174us/step - loss: 0.1770 - acc: 0.9750\n",
            "Epoch 96/150\n",
            "120/120 [==============================] - 0s 190us/step - loss: 0.1814 - acc: 0.9417\n",
            "Epoch 97/150\n",
            "120/120 [==============================] - 0s 170us/step - loss: 0.1739 - acc: 0.9750\n",
            "Epoch 98/150\n",
            "120/120 [==============================] - 0s 197us/step - loss: 0.1752 - acc: 0.9750\n",
            "Epoch 99/150\n",
            "120/120 [==============================] - 0s 191us/step - loss: 0.1692 - acc: 0.9667\n",
            "Epoch 100/150\n",
            "120/120 [==============================] - 0s 217us/step - loss: 0.1732 - acc: 0.9667\n",
            "Epoch 101/150\n",
            "120/120 [==============================] - 0s 163us/step - loss: 0.1654 - acc: 0.9750\n",
            "Epoch 102/150\n",
            "120/120 [==============================] - 0s 177us/step - loss: 0.1646 - acc: 0.9750\n",
            "Epoch 103/150\n",
            "120/120 [==============================] - 0s 225us/step - loss: 0.1644 - acc: 0.9833\n",
            "Epoch 104/150\n",
            "120/120 [==============================] - 0s 184us/step - loss: 0.1631 - acc: 0.9750\n",
            "Epoch 105/150\n",
            "120/120 [==============================] - 0s 273us/step - loss: 0.1672 - acc: 0.9750\n",
            "Epoch 106/150\n",
            "120/120 [==============================] - 0s 183us/step - loss: 0.1654 - acc: 0.9750\n",
            "Epoch 107/150\n",
            "120/120 [==============================] - 0s 180us/step - loss: 0.1646 - acc: 0.9750\n",
            "Epoch 108/150\n",
            "120/120 [==============================] - 0s 158us/step - loss: 0.1539 - acc: 0.9750\n",
            "Epoch 109/150\n",
            "120/120 [==============================] - 0s 178us/step - loss: 0.1614 - acc: 0.9583\n",
            "Epoch 110/150\n",
            "120/120 [==============================] - 0s 184us/step - loss: 0.1634 - acc: 0.9833\n",
            "Epoch 111/150\n",
            "120/120 [==============================] - 0s 158us/step - loss: 0.1521 - acc: 0.9750\n",
            "Epoch 112/150\n",
            "120/120 [==============================] - 0s 229us/step - loss: 0.1645 - acc: 0.9500\n",
            "Epoch 113/150\n",
            "120/120 [==============================] - 0s 211us/step - loss: 0.1576 - acc: 0.9833\n",
            "Epoch 114/150\n",
            "120/120 [==============================] - 0s 178us/step - loss: 0.1519 - acc: 0.9750\n",
            "Epoch 115/150\n",
            "120/120 [==============================] - 0s 155us/step - loss: 0.1515 - acc: 0.9833\n",
            "Epoch 116/150\n",
            "120/120 [==============================] - 0s 179us/step - loss: 0.1537 - acc: 0.9750\n",
            "Epoch 117/150\n",
            "120/120 [==============================] - 0s 176us/step - loss: 0.1542 - acc: 0.9667\n",
            "Epoch 118/150\n",
            "120/120 [==============================] - 0s 170us/step - loss: 0.1455 - acc: 0.9583\n",
            "Epoch 119/150\n",
            "120/120 [==============================] - 0s 180us/step - loss: 0.1409 - acc: 0.9667\n",
            "Epoch 120/150\n",
            "120/120 [==============================] - 0s 170us/step - loss: 0.1419 - acc: 0.9750\n",
            "Epoch 121/150\n",
            "120/120 [==============================] - 0s 155us/step - loss: 0.1463 - acc: 0.9833\n",
            "Epoch 122/150\n",
            "120/120 [==============================] - 0s 210us/step - loss: 0.1542 - acc: 0.9667\n",
            "Epoch 123/150\n",
            "120/120 [==============================] - 0s 205us/step - loss: 0.1449 - acc: 0.9750\n",
            "Epoch 124/150\n",
            "120/120 [==============================] - 0s 222us/step - loss: 0.1430 - acc: 0.9667\n",
            "Epoch 125/150\n",
            "120/120 [==============================] - 0s 158us/step - loss: 0.1507 - acc: 0.9750\n",
            "Epoch 126/150\n",
            "120/120 [==============================] - 0s 235us/step - loss: 0.1424 - acc: 0.9750\n",
            "Epoch 127/150\n",
            "120/120 [==============================] - 0s 212us/step - loss: 0.1387 - acc: 0.9833\n",
            "Epoch 128/150\n",
            "120/120 [==============================] - 0s 179us/step - loss: 0.1353 - acc: 0.9667\n",
            "Epoch 129/150\n",
            "120/120 [==============================] - 0s 193us/step - loss: 0.1397 - acc: 0.9583\n",
            "Epoch 130/150\n",
            "120/120 [==============================] - 0s 189us/step - loss: 0.1407 - acc: 0.9667\n",
            "Epoch 131/150\n",
            "120/120 [==============================] - 0s 230us/step - loss: 0.1368 - acc: 0.9667\n",
            "Epoch 132/150\n",
            "120/120 [==============================] - 0s 164us/step - loss: 0.1443 - acc: 0.9583\n",
            "Epoch 133/150\n",
            "120/120 [==============================] - 0s 234us/step - loss: 0.1339 - acc: 0.9750\n",
            "Epoch 134/150\n",
            "120/120 [==============================] - 0s 187us/step - loss: 0.1325 - acc: 0.9667\n",
            "Epoch 135/150\n",
            "120/120 [==============================] - 0s 191us/step - loss: 0.1316 - acc: 0.9667\n",
            "Epoch 136/150\n",
            "120/120 [==============================] - 0s 181us/step - loss: 0.1488 - acc: 0.9667\n",
            "Epoch 137/150\n",
            "120/120 [==============================] - 0s 186us/step - loss: 0.1343 - acc: 0.9750\n",
            "Epoch 138/150\n",
            "120/120 [==============================] - 0s 193us/step - loss: 0.1341 - acc: 0.9750\n",
            "Epoch 139/150\n",
            "120/120 [==============================] - 0s 189us/step - loss: 0.1368 - acc: 0.9583\n",
            "Epoch 140/150\n",
            "120/120 [==============================] - 0s 233us/step - loss: 0.1294 - acc: 0.9667\n",
            "Epoch 141/150\n",
            "120/120 [==============================] - 0s 187us/step - loss: 0.1290 - acc: 0.9667\n",
            "Epoch 142/150\n",
            "120/120 [==============================] - 0s 242us/step - loss: 0.1335 - acc: 0.9667\n",
            "Epoch 143/150\n",
            "120/120 [==============================] - 0s 181us/step - loss: 0.1268 - acc: 0.9750\n",
            "Epoch 144/150\n",
            "120/120 [==============================] - 0s 198us/step - loss: 0.1399 - acc: 0.9583\n",
            "Epoch 145/150\n",
            "120/120 [==============================] - 0s 171us/step - loss: 0.1350 - acc: 0.9750\n",
            "Epoch 146/150\n",
            "120/120 [==============================] - 0s 161us/step - loss: 0.1217 - acc: 0.9750\n",
            "Epoch 147/150\n",
            "120/120 [==============================] - 0s 177us/step - loss: 0.1227 - acc: 0.9667\n",
            "Epoch 148/150\n",
            "120/120 [==============================] - 0s 181us/step - loss: 0.1232 - acc: 0.9750\n",
            "Epoch 149/150\n",
            "120/120 [==============================] - 0s 181us/step - loss: 0.1292 - acc: 0.9833\n",
            "Epoch 150/150\n",
            "120/120 [==============================] - 0s 183us/step - loss: 0.1253 - acc: 0.9583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f17488597f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHIs37mqzl_x",
        "colab_type": "code",
        "outputId": "322749df-4ae0-4af6-987b-e0605d72374b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Test the accuracy of the model on test data\n",
        "loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)  # Evaluate the model\n",
        "print('Accuracy :%0.3f, loss:%0.3f '%(accuracy,loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy :0.967, loss:0.156 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vQt8MG-zl_0",
        "colab_type": "text"
      },
      "source": [
        "### Confusion Matrix and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU0ZhZDyzl_2",
        "colab_type": "code",
        "outputId": "45ce5bf3-77f5-4c4b-c6a6-0525d8da8ca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Lastly, report the accuracy of your model and print the Confusion Matrix\n",
        "loss, accuracy = model.evaluate(X, Y, verbose=0)  # Evaluate the model\n",
        "print('Accuracy :%0.3f, loss:%0.3f '%(accuracy,loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy :0.573, loss:0.984 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL_Db2KTzl_-",
        "colab_type": "code",
        "outputId": "9b998b74-42de-45ff-85bc-6cc09bda69a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print( confusion_matrix(Y_test.argmax(axis=1), Y_pred.argmax(axis=1)) )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  0 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}